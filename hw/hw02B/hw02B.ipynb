{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw02B.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro-hw2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# üïµÔ∏è Homework 2B: Food Safety (Continued)\n",
    "\n",
    "## ‚ÄºÔ∏è Due Date: Friday, February 13, 11:59 PM\n",
    "You must submit this assignment to Pensive by the on-time deadline, Friday, February 13, 11:59 PM. Please read the syllabus for the Slip Day policy. No late submissions beyond the details in the Slip Day policy will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to late-night requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Pensive several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support. \n",
    "\n",
    "Please read the instructions carefully when you are submitting your work to Pensive.\n",
    "\n",
    "## üí™ Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìú This Assignment\n",
    "\n",
    "In this homework, we will continue our exploration of restaurant food safety scores for restaurants in San Francisco. The main goal for this assignment is to focus more on the analysis of the dataset, building on the data cleaning we have done earlier in HW 2A. \n",
    "\n",
    "\n",
    "After this homework, you should be comfortable with:\n",
    "* Reading `pandas` documentation and using `pandas` methods,\n",
    "* Working with data at different levels of granularity,\n",
    "* Using `groupby` with different aggregation functions, and\n",
    "* Chaining different `pandas` functions and methods to find answers to exploratory questions.\n",
    "\n",
    "\n",
    "## üíØ Score Breakdown \n",
    "Question | Manual | Points\n",
    "--- | --- | ---\n",
    "1a | no | 2\n",
    "1b | no | 3\n",
    "1c | no | 3\n",
    "2a | no | 2\n",
    "2b | no | 3\n",
    "3a | yes | 3\n",
    "3b | yes | 3\n",
    "4a | no | 2\n",
    "4b | no | 3\n",
    "4c | no | 3\n",
    "4d | yes | 1\n",
    "5 | yes | 2\n",
    "Total | 4 | 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úä Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder.** Please be sure to check your results carefully.\n",
    "\n",
    "Finally, unless we state otherwise, **do not use for loops or list comprehensions**. The majority of this assignment can be done using built-in commands in `pandas` and `NumPy`. Our autograder isn't smart enough to check, but you're depriving yourself of key learning objectives if you write loops / comprehensions, and you also won't be ready for the midterm.\n",
    "\n",
    "### üêõ Debugging Guide\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Pensive, and common `pandas` errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HW 2A, we took you through the entire process of reading data from a file to perform some exploration of the data. Here, we again load the dataset that we will be using in HW 2B along with some of the columns we had added in HW 2A. For any additional context regarding the dataset, we encourage you to revisit HW 2A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = pd.read_csv('data/bus.csv', encoding='ISO-8859-1').rename(columns={\"business id column\": \"bid\"})\n",
    "bus['postal5'] = bus['postal_code'].str[:5]\n",
    "valid_zip_codes = pd.read_json(\"data/sf_zipcodes.json\")['zip_codes'].astype(\"string\")\n",
    "bus = bus[bus['postal_code'].isin(valid_zip_codes)]\n",
    "\n",
    "ins = pd.read_csv('data/ins.csv')\n",
    "ins['timestamp'] = pd.to_datetime(ins['date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "ins['bid'] = ins['iid'].str.split(\"_\", expand=True)[0].astype(int) \n",
    "\n",
    "# This code is essential for the autograder to function properly. Do not edit.\n",
    "ins_test = ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üîé Question 1: Inspecting the Inspections\n",
    "\n",
    "## üöÄ Question 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-174ed23c543ad9da",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's start by looking again at the first 5 rows of `ins` to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0fbe724a2783e33",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ins.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To better understand how the scores have been allocated, let's examine how the maximum score varies for each type of inspection. \n",
    "\n",
    "Create a `DataFrame` object `ins_score_by_type`, indexed by all the inspection types (e.g., New Construction, Routine - Unscheduled, etc.), with a single column named `max_score` containing the highest score received. Additionally, order `ins_score_by_type` by `max_score` in descending order. \n",
    "\n",
    "**Hint:** You may find the `rename` ([documentation](https://pandas.pydata.org/pandas-docs/version/2.3/reference/api/pandas.DataFrame.rename.html)) to be useful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ins_score_by_type = ...\n",
    "\n",
    "...\n",
    "ins_score_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Question 1b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Given the variability of `ins['score']` observed in 1a, let's examine the inspection scores `ins['score']` further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ins['score'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "There are a large number of inspections with a score of -1. These are probably missing values. Let's see what types of inspections have scores and which do not (score of -1). \n",
    "\n",
    "- First, define a new column `Missing Score` in `ins` where each row maps to the string `\"Yes\"` if the `score` for that business is -1 and `\"No\"` otherwise. \n",
    "\n",
    "- Then, use `groupby` to find the number of inspections for every combination of `type` and `Missing Score`. Store these values in a new column `Count`. \n",
    "\n",
    "- Finally, sort `ins_missing_score_group` by descending `Count`s. \n",
    "The result should be a `DataFrame` that looks like the one shown below.\n",
    "\n",
    "**Hint**: You may find the `map` ([documentation](https://pandas.pydata.org/pandas-docs/version/2.3/reference/api/pandas.DataFrame.map.html)) useful for defining `Missing Score`! \n",
    "\n",
    "**Note:** Again, as mentioned in Lab 2B, if you define the `agg_func` in any problem that involves `pivot_table` or `agg` in any problem that involves `groupBy`, you might encounter the following error: <br>\n",
    "\n",
    "`FutureWarning: The provided callable <built-in function (the function that you want to use)> is currently using DataFrameGroupBy.(the function you want to use). In a future version of pandas, the provided callable will be used directly. To keep current behavior, pass the string (the function you want to use) instead.`\n",
    "\n",
    "Do not panic! You can safely ignore this message for this semester. However, if you would like to avoid seeing the warning entirely, follow the instructions provided in the warning and pass the string version of the function you want to use instead. For example, if you want to use `np.min`, write `\"min\"` instead of `np.min`.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th></th>      <th>Count</th>   </tr>\n",
    "    <tr style=\"text-align: right;\">      <th>type</th>      <th>Missing Score</th>      <th></th>   </tr>    <tr align=\"right\"> <tbody>    \n",
    "    <tr  align=\"right\">      <th>Routine - Unscheduled</th>      <th>No</th>      <td>14031</td>         </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>        </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>       </tr>    </tbody> </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ins['Missing Score'] = ...\n",
    "ins_missing_score_group = ...\n",
    "\n",
    "\n",
    "ins_missing_score_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Question 1c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `groupby` to perform the analysis above gave us a `DataFrame` that wasn't the most readable at first glance. There are better ways to represent the information above that take advantage of the fact that we are looking at combinations of two variables. It's time to pivot (pun intended)!\n",
    "\n",
    "Create a `DataFrame` that looks like the one below, and assign it to the variable `ins_missing_score_pivot`. \n",
    "\n",
    "You'll want to use the `pivot_table` method of the `DataFrame` class, which you can read about in the `pivot_table` [documentation](https://pandas.pydata.org/pandas-docs/version/2.3/reference/api/pandas.DataFrame.pivot_table.html). \n",
    "\n",
    "- Once you create `ins_missing_score_pivot`, add another column titled `Proportion Missing`, which contains the proportion of missing scores within each `type`. \n",
    "\n",
    "- Then, sort `ins_missing_score_pivot` in descending order of `Proportion Missing`. Reassign the sorted `DataFrame` back to `ins_missing_score_pivot`.\n",
    "\n",
    "**Hint:** Consider what happens if no values correspond to a particular combination of `Missing Score` and `type`. Looking at the documentation for `pivot_table`, is there any function argument that allows you to specify what value to fill in?\n",
    "\n",
    "If you've done everything right, you should observe that inspection scores appear only to be assigned to `Routine - Unscheduled` inspections and that `ins_missing_score_pivot` looks like the `DataFrame` below:\n",
    "\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th>Missing Score</th>      <th>No</th>      <th>Yes</th>      <th>Proportion Missing</th>    </tr>    <tr style=\"text-align: right;\">      <th>type</th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>Administrative or Document Review</th>      <td>0</td>      <td>4</td>      <td>1.000000</td>    </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    \n",
    "    </tbody></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ins_missing_score_pivot = ...\n",
    "\n",
    "...\n",
    "\n",
    "ins_missing_score_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable for inspection types such as `New Ownership` and `Complaint` to have no associated inspection scores, but you might be curious why there are no inspection scores for the `Reinspection/Followup` inspection type. Later in the HW, we will examine these `Reinspection/Followup` inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Question 2: Joining Data Across Tables\n",
    "\n",
    "In this question, we will start to connect data across multiple tables. We will be using the `pd.merge` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## üöÄ Question 2a\n",
    "\n",
    "Let's figure out which restaurants had the lowest scores. Before we proceed, filter out missing scores from `ins` so that negative scores don't influence our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = ins[ins[\"score\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We'll start by creating a new `DataFrame` called `ins_named`. `ins_named` should be exactly the same as `ins`, except that it should have the name and address of every business, as determined by the `bus` `DataFrame`. \n",
    "\n",
    "**Hint**: Use the `DataFrame` method `merge` to join the `ins` `DataFrame` with the appropriate portion of the `bus` `DataFrame`. See the [documentation](https://pandas.pydata.org/pandas-docs/version/2.3/user_guide/merging.html) for guidance on how to use `merge` function to combine two `DataFrame` objects. The first few rows of `ins_named` `DataFrame` are shown below:\n",
    "\n",
    "<img src=\"pics/2a.png\" width=\"1080\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ins_named = ...\n",
    "\n",
    "...\n",
    "ins_named.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## üöÄ Question 2b\n",
    "\n",
    "Look at the 10 businesses in `ins_named` with the lowest scores. Order `ins_named` by each business's minimum score in ascending order. Use the business names in ascending order to break ties. The resulting `DataFrame` should look like the table below.\n",
    "\n",
    "This one is pretty challenging! Don't forget to rename the `score` column. \n",
    "\n",
    "**Hint**: The `agg` function can accept a dictionary as an input. See the `agg` [documentation](https://pandas.pydata.org/pandas-docs/version/2.3/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html). Additionally, when thinking about what aggregation functions to use, ask yourself: \"*What value would be in the `name` column for each entry across the group? Can we select just one of these values to represent the whole group?*\"\n",
    "\n",
    "As usual, **YOU SHOULD NOT USE LOOPS OR LIST COMPREHENSIONS**. Try to break down the problem piece by piece instead, gradually chaining together different `pandas` functions. Feel free to use more than one line!\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>name</th>      <th>min score</th>    </tr> \n",
    "    <tr  align=\"right\">  <th align=\"right\">bid</th>      <th></th>      <th></th>    </tr> </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>86718</th>      <td>Lollipot</td>      <td>45</td>    </tr>  \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>    </tr> \n",
    "  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "ten_lowest_scoring = ... \n",
    "\n",
    "# DO NOT USE LIST COMPREHENSIONS OR LOOPS OF ANY KIND!!!\n",
    "\n",
    "...\n",
    "\n",
    "ten_lowest_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üåÆ Question 3: `pandas` Potpourri\n",
    "\n",
    "In this question, we ask you to describe `pandas` operations and explain specific concepts using `ins_named`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üåÆ Question 3a\n",
    "\n",
    "Consider the chained `pandas` statement below:\n",
    "\n",
    "`q3a_df = ins_named[ins_named[\"name\"].str.lower().str.contains(\"taco\")].groupby(\"bid\").filter(lambda sf: sf[\"score\"].max() > 95).agg(\"count\")`\n",
    "\n",
    "We can decompose this statement into three parts:\n",
    "\n",
    "```\n",
    "temp1 = ins_named[ins_named[\"name\"].str.lower().str.contains(\"taco\")]\n",
    " \n",
    "temp2 = temp1.groupby(\"bid\").filter(lambda sf: sf[\"score\"].max() > 95)\n",
    " \n",
    "q3a_series = temp2.agg(\"count\")\n",
    "```\n",
    "\n",
    "For each line of code above, write one sentence describing what the line of code accomplishes. Feel free to create a cell to see what each line does. In total, you'll write three sentences.\n",
    "\n",
    "An example answer will look like the following: \"`temp1` creates a ... `temp2` transforms `temp1` by ... Finally, `q3a_df` results in a `DataFrame` that ... \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üåÆ Question 3b\n",
    "\n",
    "Consider `temp1`, `temp2`, and `q3a_series` from the previous problem. What is the granularity of each `DataFrame`? Explain your answer in no more than three sentences.\n",
    "\n",
    "**Note**: For more details on what the granularity of a `DataFrame` means, feel free to check the [course notes](https://ds100.org/course-notes/eda/#granularity)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Question 4: Missing Inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our inspection data, we are given the `type` of each inspection. These categories were lightly investigated in Question 1, centered on the number of missing scores within each `type`. Since the `timestamp` and `score` for each inspection are also provided, we can do a more interesting analysis relating the `score` and `timestamp` of specific types of inspections. \n",
    "\n",
    "Specifically, in Question 4, we are interested in the possible relationship between inspections of the `type` \"Routine - Unscheduled\" and \"Reinspection/Followup\" (the two most frequent inspection types in our dataset). We might guess that a follow-up (\"Reinspection/Followup\") inspection occurs more frequently when an initial (\"Routine - Unscheduled\") inspection receives a low score. To confirm this hunch, let‚Äôs investigate the rate of follow-up inspections for different initial scores. To simplify your analysis, we have provided a new `DataFrame` (`reinspections`). \n",
    "\n",
    "- `reinspections` contains every \"Routine - Unscheduled\" inspection, along with the relevant `bid` and `name` associated with the initial inspection. \n",
    "- `routine timestamp` indicates when the initial inspection occurred. \n",
    "- `routine score` is the score that the initial inspection received. \n",
    "- `day difference` is the number of days between the initial inspection and a follow-up inspection if done within one year. \n",
    "    \n",
    "Some initial inspections did not have any follow-up inspections within one year. In these cases, `day difference` is assigned a filler value of -1.\n",
    "\n",
    "Run the cell below to load in `reinspections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinspections = pd.read_csv('data/reinspections.csv')\n",
    "reinspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4a\n",
    "First, create a new `Boolean` column `recent reinspection?` that indicates whether a follow-up inspection occurred within 62 days inclusive (~2 months) of an initial inspection. Remember that `day difference` is assigned a filler value of -1 if initial inspections did not have any follow-up inspections within one year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "reinspections['recent reinspection?'] = ...\n",
    "reinspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4b\n",
    "To simplify our analysis, we will assign `routine score`s to buckets. Buckets are similar to the bins of a histogram. Each bucket contains all scores that fall in a particular range. Below we have defined the function `bucketify` and used `bucketify` to create a new column in `reinspections` called `score buckets` using the `map` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Just run the cell, Do not edit\n",
    "def bucketify(score):\n",
    "    if score < 65: \n",
    "        return '0 - 65'\n",
    "    elif score < 70:\n",
    "        return '65 - 69'\n",
    "    elif score < 75:\n",
    "        return '70 - 74'\n",
    "    elif score < 80:\n",
    "        return '75 - 79'\n",
    "    elif score < 85:\n",
    "        return '80 - 84'\n",
    "    elif score < 90:\n",
    "        return '85 - 89'\n",
    "    elif score < 95:\n",
    "        return '90 - 94'\n",
    "    else:\n",
    "        return '95 - 100'\n",
    "        \n",
    "reinspections['score buckets'] = reinspections['routine score'].map(bucketify)\n",
    "\n",
    "reinspections.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "To continue our analysis, remove all rows whose `score buckets` contain less than 120 rows. Assign `reinspections_filtered` to this new `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "reinspections_filtered = ...\n",
    "\n",
    "reinspections_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4c\n",
    "\n",
    "To conclude our analysis, use `reinspections_filtered` to generate a `DataFrame` with the **proportion** of initial inspections within each bucket that were reinspected within 62 days, along with the total **count** of initial inspections included in each bucket. Sort this `DataFrame` by **ascending** counts. Assign this new `DataFrame` to `reinspection_proportions`.\n",
    "\n",
    "`reinspection_proportions` should look like the `DataFrame` below.\n",
    "\n",
    "**Hint:** You may find the examples under the `agg` ([documentation](https://pandas.pydata.org/pandas-docs/version/2.3/reference/api/pandas.DataFrame.rename.html)) to be useful! \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\" >  <thead>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>recent reinspection?</th>   <th></th> </tr>    \n",
    "    <tr style=\"text-align: right;\">      <th></th>      <th>proportion</th>      <th>count</th>    </tr>    \n",
    "    <tr style=\"text-align: right;\">      <th>score buckets</th>      <th></th>      <th></th>     </tr>  </thead>  <tbody>    \n",
    "    <tr  align=\"right\">      <th>65 - 69</th>      <td>0.409836</td>      <td>122</td>    </tr>    \n",
    "    <tr  align=\"right\">      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    \n",
    "    </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "reinspection_proportions = ...\n",
    "\n",
    "reinspection_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "--- \n",
    "## üöÄ Question 4d\n",
    "\n",
    "Do you notice any trends? Are your results consistent with your prior knowledge about restaurants that receive high or low health inspection scores? Answer in the cell below. Answer in no more than 3 sentences.\n",
    "\n",
    "**This question is graded on effort, there is no one \"correct\" answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# ü§ñ Question 5: Open-Ended Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first **open-ended question** of the semester. The intent behind these **open-ended homework questions** is to provide you with new opportunities to think creatively as data scientists.\n",
    "\n",
    "> If you have any feedback on this open-ended question, or any other homework question in Data 100, we encourage you to share your thoughts using the [content feedback form](https://forms.gle/3g1LxtTwquwZE8rr6). You can also post to Ed.\n",
    "\n",
    "Grading on open-ended questions is simple: **Clear evidence of thoughtfulness and effort will always receive full credit**. If your response is especially well-developed or creative, we may ask for permission to share it with the rest of the class so others can be inspired by your work! Underdeveloped ideas will receive half credit. Trivial or missing responses will receive no credit. We expect the vast majority of students to receive full credit.\n",
    "\n",
    "Run the cell below to watch a video from Josh introducing the open-ended question. [Here](https://www.youtube.com/watch?v=s9edArRkssM&list=PLQCcNQgUcDfr2CvC5ZaQqyxf0wnzqwYo4&index=1) is a link to the YouTube video in case the video does not render for you. You must be signed into your **Berkeley account on YouTube** in order for the video to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"s9edArRkssM\", list = 'PLQCcNQgUcDfr2CvC5ZaQqyxf0wnzqwYo4', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SETUP**: You are a data scientist working for the [San Francisco Department of Public Health](https://www.sf.gov/departments--department-public-health). Your manager has been allocated a small budget to address the health inspection scores of the restaurants in **one** neighborhood in San Francisco. Neighborhoods are defined by their postal code `postal5`.\n",
    "\n",
    "**TASK**: Your task is to recommend **one neighborhood** (`postal5`) where funding would have the biggest **impact**.\n",
    "\n",
    "> Food for thought: Assume your manager has a ChatGPT Pro subscription and could easily ask the question above without reaching out to you for help. What's your added value over their ChatGPT Pro subscription? \n",
    "\n",
    "> Also, you might be wondering, what is the definition of \"impact\"? This is up to you to define and defend. There is no single correct answer. Remember that a major part of data science is defining what success actually means.\n",
    "\n",
    "> Finally, you might also think about where restaurant inspection scores actually come from. Is an inspection score a ground truth about food safety, or an approximation? Does a particular score mean the same thing for two different restaurants? How might the public health department have decided on the inputs to the inspection score? Could there be bias in how scores are recorded? These kinds of questions may be helpful to think about as you define \"impact\".\n",
    "\n",
    "For this task, you may use: \n",
    "- The `reinspections` and `bus` `DataFrame`s.\n",
    "- Any `pandas` syntax covered in class.\n",
    "- (Optional) External resources (e.g., AI/LLMs, websites, datasets, or other Python libraries/packages).\n",
    "\n",
    "Important exception to existing course policies: **<u>FOR THIS QUESTION ONLY</u>**, you are allowed to [vibe code](https://en.wikipedia.org/wiki/Vibe_coding). In other words, the code you use to generate responses can be generated by a large-language model (LLM), like Gemini or ChatGPT. However, the most important component of this question is not the code‚Äîit's the presentation and persuasiveness of your results. **If you copy-and-paste default output from an LLM on this question, there is a good chance that your submission will look identical or near-identical to many other students**. While we expect many answers to this question to have similarities, obvious default output will receive no credit. Spend time thinking about the presentation of your results. \n",
    "\n",
    "> **Disclaimer**: As Data Science students, you should be aware of important limitations and broader considerations when it comes to the use of LLMs. \n",
    "> - LLMs do not guarantee factual accuracy and they are known to hallucinate (generate fabricated or misleading information). \n",
    "> - LLMs are trained on large datasets that can reflect and reproduce biases in race, gender, culture, and ideology. \n",
    "> - The use of LLMs may involve the sharing of sensitive and personal information, \n",
    "\n",
    "\n",
    "\n",
    "**Your recommendation should consist of the following**:\n",
    "1. A **single** `pandas` `DataFrame` that could be printed on a single sheet of paper in a standard sized font. As a starting point, you might like to think about summary statistics that reflect your chosen definition of \"impact\". But remember, this question is open-ended and the contents of the `DataFrame` are ultimately **up to you**.\n",
    "2. A **write up of 6‚Äì10 sentences** stating your definition of \"impact\", your recommended neighborhood, an explanation of why you think your chosen neighborhood has the highest potential for impact, and **one possible counterargument** as to why someone may disagree with your recommendation . Your explanation as well as your counterargument must reference the contents of your `DataFrame`. Make it obvious and clear why the contents of your `DataFrame` support your recommendation.\n",
    "\n",
    "> Keep in mind that your manager is interested in your recommendation and the evidence you collected to support that recommendation. Don't spend more than a sentence or two explaining how you generated your `DataFrame` (e.g., no need to write \"I used the X library to construct the Y column of my DataFrame.\"). \n",
    "\n",
    "> Focus on answering your manager's question, rather than explaining the detailed steps needed to answer the question (e.g., \"The Y column of my DataFrame shows Z, which is why neighborhood 12345 has the greatest potential for impact.\").\n",
    "\n",
    "\n",
    "> **IMPORTANT**: If you have any questions, please read through the [**FAQS**](https://docs.google.com/document/d/1Dcfyl7MjmR6wKgsCxbL31Bz82JMb6w9g68EcK0dpYuc/edit?usp=sharing) first. If you can't find the answer to your question there, feel free to ask your question on Ed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBMISSION INSTRUCTIONS\n",
    "\n",
    "**Please read these carefully**\n",
    "1. In the 2 scratch cells below, feel free to write any code you need to create your **single** `pandas` `DataFrame`.\n",
    "2. In the next cell, assign your single `pandas` `DataFrame` to `final_df`.\n",
    "3. Run the code in the next cell to save your `DataFrame` as an image.\n",
    "4. Set `image_created` to `True`.\n",
    "5. **Comment out all your code** in both scratch cells (this means adding `#` at the start of each line of code) **AND** the `final_df` cell. If you do not comment out your code, the autograder will attempt to run your code, and this can cause unexpected problems.\n",
    "6. Set `commented_out` to `True`.\n",
    "7. In the markdown cell, complete your write-up in 6‚Äì10 sentences.\n",
    "\n",
    "Watch the video below explaining the submission instructions. [Here](https://www.youtube.com/watch?v=xTMqNPNCraE&list=PLQCcNQgUcDfr2CvC5ZaQqyxf0wnzqwYo4&index=2) is a link to the YouTube video in case the video does not render for you. You must be signed into your **Berkeley account on YouTube** in order for the video to render. \n",
    "\n",
    "Note 1: The video quality is poor, but the notebook in the video is the same as the one you are currently working on.\n",
    "\n",
    "Note 2: The video forgets to mention to also **comment out the** `final_df` **code cell** for step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"xTMqNPNCraE\", list = 'PLQCcNQgUcDfr2CvC5ZaQqyxf0wnzqwYo4', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# SCRATCH CELL 1\n",
    "# (Optional) Feel free to import any extra libraries or packages here\n",
    "# However, you do not need to import any extra libraries to answer the question and receive full credit\n",
    "# Remember to comment out your code for step 5\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# SCRATCH CELL 2\n",
    "# Feel free to do your rough work here\n",
    "# Remember to comment out your code for step 5\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Set your desired DataFrame to final_df\n",
    "# Remember to comment out this code for step 5\n",
    "final_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this cell once you're happy with your final_df\n",
    "# Set image_created = True once you've run this once\n",
    "image_created = False\n",
    "...\n",
    "\n",
    "# DO NOT EDIT THE CODE BELOW\n",
    "if not image_created:\n",
    "    !pip install dataframe_image\n",
    "    import dataframe_image as dfi\n",
    "    \n",
    "    dfi.export(final_df, \"final_df.png\", table_conversion=\"matplotlib\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"final_df.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Set commented_out = True once you have commented out all your code from the scratch cells and the final_df cell\n",
    "\n",
    "commented_out = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Summary of Inspections Data\n",
    "\n",
    "We have done a lot in this homework! \n",
    " \n",
    "- Broke down the inspection scores in detail using `groupby` and `pivot_table`.\n",
    "- Joined the business and inspection data and identified restaurants with the worst ratings.\n",
    "- Took a deep dive into understanding any trends between an inspection score and reinspection frequency.\n",
    "\n",
    "Over the course of this 2-part homework, we hope you have become more familiar with `pandas` - in terms of identifying when to use particular functions, how they work, when they can support EDA - as well as with EDA and Data Cleaning, as part of the broader Data Science Lifecycle. These tools will serve you well as a data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Homework 2B! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scout say hi :)\n",
    "\n",
    "<img src = \"pics/Scout.PNG\" width = \"400\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/4AKg1jycvvPGJ5t67). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the HW 2B Coding assignment on Pensive, Pensive will automatically submit a PDF file with your written answers to the HW 2B Written assignment. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder-gradescope/#why-does-the-last-grader-export-cell-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide.\n",
    "\n",
    "**Important**: Please check that your written responses were generated and submitted correctly to the HW 2B Written Assignment.\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that the PDF for HW 2B written answers was generated/submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(len(ins_score_by_type.columns) == 1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_score_by_type.columns[0] == 'max_score')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(set(ins_score_by_type.index) == set(ins['type'].unique()))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(('New Ownership - Followup', 'Yes') in ins_missing_score_group.index)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_missing_score_group.columns[0] == 'Count')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(len(ins_missing_score_group) == 16)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_missing_score_group['Count'].iloc[0] == 14031)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(type(ins_missing_score_pivot) == pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_missing_score_pivot['Yes']['Administrative or Document Review'] == 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(ins_missing_score_pivot['No']['Administrative or Document Review'] == 0)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'name' in ins_named and 'address' in ins_named\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ins_named[ins_named['Missing Score'] == True].shape[0] == 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ins_named.shape == (13625, 9)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ins_named.reset_index()['date'].equals(ins[ins['score'] > 0].merge(bus[['bid', 'name', 'address']], left_on='bid', right_on='bid').reset_index()['date'])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert ten_lowest_scoring.shape == (10, 2)\n>>> assert len(ten_lowest_scoring.index.names) == 1\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> ten_lowest_scoring.iloc[0, 0] == 'Lollipot'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert reinspections.shape == (14077, 6)\n>>> assert (reinspections.columns == ['bid', 'routine timestamp', 'routine score', 'name', 'day difference', 'recent reinspection?']).all()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert (reinspections_filtered.columns == ['bid', 'routine timestamp', 'routine score', 'name', 'day difference', 'recent reinspection?', 'score buckets']).all()\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert reinspection_proportions.shape == (7, 2)\n>>> assert reinspection_proportions.columns[0] == ('recent reinspection?', 'proportion')\n>>> assert reinspection_proportions.columns[1] == ('recent reinspection?', 'count')\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert image_created == True\n>>> assert commented_out == True\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
